name: Ejecutar script de Scrape

on:
  schedule:
    # Se ejecuta cada martes y viernes a las 09:00 UTC (10:00 CEST en verano)
    - cron: '0 9 * * 2,5'
  workflow_dispatch: # Permite ejecutar el workflow manualmente desde la interfaz de GitHub

jobs:
  scrape_and_commit:
    runs-on: ubuntu-latest
    permissions:
      contents: write # Permiso necesario para escribir en el repositorio (para el commit)

    steps:
      - name: Checkout del repositorio
        uses: actions/checkout@v4 # Clona tu repositorio en el runner

      - name: Configurar Python
        uses: actions/setup-python@v5 # Configura el entorno de Python
        with:
          python-version: '3.10' # Especifica la versión de Python a usar

      - name: Instalar dependencias
        run: |
          python -m pip install --upgrade pip # Actualiza pip
          pip install pandas lxml html5lib unidecode cloudscraper bs4  # Instala las librerías necesarias para tu script

      - name: Ejecutar script de Scrape
        # Asegúrate de que la ruta 'scripts/scrape.py' sea correcta
        run: python scripts/cleaning_merged.py

      - name: Commit y Push del archivo CSV generado
        uses: stefanzweifel/git-auto-commit-action@v5 # Acción para hacer commit y push automáticamente
        with:
          commit_message: "chore: Actualizar datos scrapeados [skip ci]" # Mensaje del commit
          branch: main # Rama a la que se hará el commit
          # IMPORTANTE: Cambia 'data/scraped_data.csv' si tu script genera un archivo diferente o en otra ubicación
          file_pattern: ./data/player_stats_cleaned_2024_2025.csv
          commit_user_name: GitHub Actions Bot
          commit_user_email: actions@github.com
          commit_author: GitHub Actions Bot <actions@github.com>
